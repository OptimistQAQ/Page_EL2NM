<!DOCTYPE html>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EL2NM: Extremely Low-light Noise Modeling Through Diffusion Iteration</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  </head>

  <style>
  .section-title {
    font-family: "monospace"
  }
  .authors {
    font-family: "monospace";
  }
  h1, h2, h3, h4, h5, h6, p {
    font-family: "monospace";
  }
  </style>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-4">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h1 style="font-family: Lato;">EL2NM: Extremely Low-light <br> Noise Modeling Through Diffusion Iteration</h1>
            <h4 style="color:#5a6268;">CVPR Workshops 2024</h4>
            <hr>
            <h5> <a href="https://optimistqaq.github.io/" target="_blank">Jiahao Qin</a><sup>1</sup>, 
                Pinle Qin<sup>*, 1</sup>, Rui Chai<sup>1</sup>, Jia Qin<sup>1</sup>, Zanxia Jin<sup>1</sup>
            </h5>
            <p style="font-size: large; font-weight: 300;">
              <sup>1</sup>North University of China &nbsp;&nbsp;
            </p>
            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://openaccess.thecvf.com/content/CVPR2024W/MIPI/html/Qin_EL2NM_Extremely_Low-light_Noise_Modeling_Through_Diffusion_Iteration_CVPRW_2024_paper.html" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper (CVPRW 2024)</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/OptimistQAQ/EL2NM" role="button"  target="_blank">
                  <i class="fa fa-github"></i> Code (CVPRW 2024)</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://kristinamonakhova.com/starlight_denoising/#dataset" role="button">
                    <i class="fa fa-cloud-download"></i> Dataset (Starlight)</a> </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
          <p class="text-justify">Low-light Original Denoising (LOD) is a challenging task in Computational Photography (CP). 
            The low number of photons in low light environments makes imaging very difficult. 
            The most difficult step in LOD is to establish a noise model under low light. 
            Currently, there are numerous approaches aim to noise modeling, however the noise established have significant differences from real noise due to the highly intricate distribution of noise. 
            Towards this goal, this paper proposes an Extremely Low-light Noise Modeling (EL2NM) approach, which designs an original image condition constraint module and a multi-noise fusion module to generate complex noise consistent with real scenes. 
            In order to satisfy the complex noise distribution in low-light environments instead of just Gaussian noise, we integrate various noises into cold diffusion to establish a realistic noise generation model for extremely low-light environments. 
            At the same time, to avoid the image semantic misinterpret during the reverse diffusion process, we propose to use conditional image to guide noise generation of the diffusion model.  
            Extensive experiments demonstrate that our proposed method EL2NM exhibits excellent performance in extremely low-light environments and achieves the state-of-the-art on Starlight Dataset. 
          </p>
          <!-- <img class="img-fluid" src="./images/pipeline.png" alt="teaser" width="90%"> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Introduction</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">Inspired by recent advancements in conditional diffusion models and cold diffusion models, we introduces a new noise modeling approach, EL2NM, building upon the work of Kristina Monakhova. EL2NM employs a series of refinement steps to convert complex noise distributions into empirical data distributions, akin to Langevin dynamics. At its core lies the U-Net architecture, utilized to train the noise model and iteratively produce noise outputs. The U-Net architecture, adapted from SR3, is modified in this work to accommodate conditional image generation. The sampling technique of the cold diffusion model is applied iteratively to generate the final noise image.</p>
          <!-- <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Contributions</p> -->
          <p class="text-justify">
            Our main contributions are summarized as follows:
            <ol>
              <li style="text-align: left;font-family: Lato;">We proposes a novel approach for noise modeling in low-light environments. By combining the conditional diffusion model with the cold diffusion model, the method employs the conditional diffusion model to control the generation of conditional images and integrates the cold diffusion model to introduce a wider range of noise distributions. Through iterative refinement, the approach achieves the creation of high-quality noise images. Importantly, this study represents the first application of diffusion models in the domain of noise modeling for low-light images.</li> 
              <!-- <img class="img-fluid" src="./images/teaser.png" alt="teaser" width="456"> -->
              <br>
              <img class="img-fluid" src="./images/overview.png" alt="overview" width="706">
              <br><br>
              <li style="text-align: left;font-family: Lato;">We extends the application of the conditional diffusion model to noise image generation. The EL2NM method involves an iterative subdivision technique to generate noise images in low-light conditions. It departs from an understanding of the physical processes involved, sidestepping the need for adversarial training, yet yielding high-quality noise images for dim-light scenarios.</li>
              <br>
              <img class="img-fluid" src="./images/wangluo.png" alt="wangluo" width="706">
              <br><br>
              <li style="text-align: left;font-family: Lato;">Experimental results affirm that the proposed approach exhibits a high level of advancement in both quantitative and qualitative evaluations. It effectively generates superior quality noise images. Moreover, the method demonstrates remarkable performance on the Starlight Dataset, showcasing its competitiveness in the field.</li>
            </ol>
          </p>
          <br>
          <!-- <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;"> Summary of Differences</p> -->
          <!-- <details><summary>Click to get details</summary> -->
          <!-- <ol>
            <li style="text-align: left;font-family: Lato;">We develop a high-quality image acquisition protocol to improve the data quality of paired real data. Based on the protocol, we build a low-light raw image denoising dataset, which promotes the reliability of data mapping.</li> 
            <li style="text-align: left;font-family: Lato;">We analyze the property of SNA and extend the application strategy based on the real read noise samples. The extended design can promote denoised images with clear details.</li>
            <li style="text-align: left;font-family: Lato;">We develop the linear dark shading model and present an in-depth analysis of the robustness and generalizability of DSC. We further explore the extension of DSC on physics-based noise modeling, which brings huge denoising performance improvements.</li>
            <li style="text-align: left;font-family: Lato;">We evaluate our methods on more datasets and conduct more extensive experiments to show the potential widespread usage of our methods.</li>
          </ol> -->
          <!-- </details> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Performance</h3>
            <hr style="margin-top:0px">
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Comparison</p>
            <h5>Quantitative Results</h5>
            <img class="img-fluid" src="./images/table_a.png" width="50%">
            <p class="text-justify">
              This study compared the proposed noise model with previous works, with each row representing a different noise modeling method. The experiments demonstrated that the proposed method is capable of generating noise images similar to the Starlight method, yielding higher quality noise images.
            </p>
            <br>
            <h5>Public Datasets (Starlight)</h5>
            <img class="img-fluid" src="./images/results_Starlight.png" width="100%">
            <p class="text-justify">
              <br> 
              This study compared commonly used noise modeling methods and demonstrated the noise images generated by the proposed method and baseline methods. 
              We found that our method EL2NM is more stable than the starlight and can generate realistic noisy image in low light environments.
              We compared the baseline Starlight model based on GAN networks, the non-deep low-light noise model ELD, as well as two deep learning-based noise models, CA-GAN and Noise Flow. 
              Both Noise Flow and CA-GAN miss the significant banding noise present in real noisy clips. ELD miss the quantizaion noise. 
              The EL2NM method exhibited good performance on this dataset. 
              Qualitative performance indicators are presented in Table, indicating that KL divergence computed by EL2NM was comparable to the baseline.
            </p>
            <br>
            <br>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Ablation Study</p>
            <h5>Quantitative Results</h5>
            <img class="img-fluid" src="./images/table_b.png" width="50%">
            <p class="text-justify">
              We compared the method with only conditional diffusion model and the method with only cold diffusion model with our method, and the results show that our method can better establish the noise model.
            </p>
            <br>
            <h5>Visual Results</h5>
            <img class="img-fluid" src="./images/result_abl.png">
            <p class="text-justify">
              We can see that when only the conditional diffusion model is used, the image does not recover noise information and some image information is lost. When there is only a cold diffusion model, the noise in image restoration is not complete enough, and the visual effect quality is not high. When combined, it can generate noise images in weak light environments more completely while ensuring the stability of the generation.
            </p>
        </div>
        
      </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@InProceedings{Qin_2024_CVPR,
  author    = {Qin, Jiahao and Qin, Pinle and Chai, Rui and Qin, Jia and Jin, Zanxia},
  title     = {EL2NM: Extremely Low-light Noise Modeling Through Diffusion Iteration},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month     = {June},
  year      = {2024},
  pages     = {1085-1094}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>
  <footer class="text-center" style="margin-bottom:10px">
    <br>
      <p style="text-align:center;font-size:small;">
        Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template. <br>
        本站访客数<span id="busuanzi_value_site_uv"></span>人次<br>
        本文总阅读量<span id="busuanzi_value_page_pv"></span>次
      </p>
  </footer>

</body>
</html>
